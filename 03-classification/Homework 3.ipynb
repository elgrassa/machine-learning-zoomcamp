{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2294f3af-da34-40b2-9515-9b7cb179f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faab13c4-e597-4b30-beed-69b355314d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents after extracting bank+marketing.zip:\n",
      "['bank-names.txt', 'bank.csv', 'bank.zip', 'bank', 'bank-full.csv', 'bank-additional.zip']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download the zip file from the URL\n",
    "url = 'https://archive.ics.uci.edu/static/public/222/bank+marketing.zip'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a new folder to extract the files into\n",
    "extraction_folder = 'bank_marketing_files'\n",
    "if not os.path.exists(extraction_folder):\n",
    "    os.makedirs(extraction_folder)\n",
    "\n",
    "# Step 2: Extract the contents of the main zip file into the new folder\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(extraction_folder)\n",
    "\n",
    "# Step 3: Check the contents of the folder to verify files\n",
    "print(\"Contents after extracting bank+marketing.zip:\")\n",
    "print(os.listdir(extraction_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c533da6c-a8f9-4820-bfb6-8e30be37709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.zip extracted successfully into 'bank' folder.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract 'bank.zip' from the new folder\n",
    "bank_zip_path = os.path.join(extraction_folder, 'bank.zip')\n",
    "bank_extraction_folder = os.path.join(extraction_folder, 'bank')\n",
    "\n",
    "with zipfile.ZipFile(bank_zip_path, 'r') as bank_zip:\n",
    "    bank_zip.extractall(bank_extraction_folder)\n",
    "\n",
    "# Step 5: Check the contents of the extracted 'bank/' folder\n",
    "if not os.path.exists(bank_extraction_folder):\n",
    "    os.makedirs(bank_extraction_folder)\n",
    "\n",
    "if os.path.exists(bank_zip_path):\n",
    "    with zipfile.ZipFile(bank_zip_path, 'r') as bank_zip:\n",
    "        bank_zip.extractall(bank_extraction_folder)\n",
    "    print(\"bank.zip extracted successfully into 'bank' folder.\")\n",
    "else:\n",
    "    print(\"bank.zip not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4fdaa9-993a-4063-8ca2-5d111a994934",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full_path = os.path.join(bank_extraction_folder, 'bank-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e985f81-2d11-4087-b12c-393db93dc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load the data\n",
    "df = pd.read_csv(bank_full_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d544221-5b69-4f8d-9275-239563fa7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n"
     ]
    }
   ],
   "source": [
    "# Step 6: Display the first few rows to verify\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe7dd9b-ddff-470b-96f6-19bc69e03cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Select only the required columns\n",
    "columns = [\n",
    "    'age', 'job', 'marital', 'education', 'balance', 'housing', 'contact',\n",
    "    'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73badc81-ddcc-4563-ac8d-f781f2fe97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d09030e-e799-46e6-97bc-8ceeecef2d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "balance      0\n",
      "housing      0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "y            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Check if there are any missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5939dd9d-0cea-4f93-890e-27c9e6ba501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most frequent observation (mode) for education is: secondary\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Find the most frequent observation (mode) for the 'education' column\n",
    "education_mode = df['education'].mode()[0]\n",
    "print(f\"\\nThe most frequent observation (mode) for education is: {education_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d55455-6a2a-4b78-96c3-5685aa720e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 27126\n",
      "Validation set size: 9042\n",
      "Test set size: 9043\n",
      "First few rows of X_train:\n",
      "       age            job  marital  education  balance housing   contact  day  \\\n",
      "6377    45   entrepreneur  married    primary     -100     yes   unknown   27   \n",
      "17236   29       services   single  secondary      166      no  cellular   28   \n",
      "4490    31         admin.   single  secondary      121     yes   unknown   20   \n",
      "24231   40  self-employed   single   tertiary     1693     yes  cellular   17   \n",
      "3978    28       services   single  secondary      317     yes   unknown   16   \n",
      "\n",
      "      month  duration  campaign  pdays  previous poutcome  \n",
      "6377    may       240         6     -1         0  unknown  \n",
      "17236   jul       108         8     -1         0  unknown  \n",
      "4490    may       187         1     -1         0  unknown  \n",
      "24231   nov       353         1     -1         0  unknown  \n",
      "3978    may        21         3     -1         0  unknown  \n",
      "Correlation matrix:\n",
      "               age   balance       day  duration  campaign     pdays  previous\n",
      "age       1.000000  0.097783 -0.009120 -0.004648  0.004760 -0.023758  0.001288\n",
      "balance   0.097783  1.000000  0.004503  0.021560 -0.014578  0.003435  0.016674\n",
      "day      -0.009120  0.004503  1.000000 -0.030206  0.162490 -0.093044 -0.051710\n",
      "duration -0.004648  0.021560 -0.030206  1.000000 -0.084570 -0.001565  0.001203\n",
      "campaign  0.004760 -0.014578  0.162490 -0.084570  1.000000 -0.088628 -0.032855\n",
      "pdays    -0.023758  0.003435 -0.093044 -0.001565 -0.088628  1.000000  0.454820\n",
      "previous  0.001288  0.016674 -0.051710  0.001203 -0.032855  0.454820  1.000000\n",
      "\n",
      "The two features with the highest correlation are: ('pdays', 'previous')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split data into train (60%) and temp (40%) using seed 42\n",
    "train_data, temp_data = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Step 2: Further split temp into validation (20%) and test (20%)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Ensure the target value 'y' is not included in the feature set\n",
    "X_train = train_data.drop(columns=['y'])\n",
    "y_train = train_data['y']\n",
    "\n",
    "X_val = val_data.drop(columns=['y'])\n",
    "y_val = val_data['y']\n",
    "\n",
    "X_test = test_data.drop(columns=['y'])\n",
    "y_test = test_data['y']\n",
    "\n",
    "# Check to confirm y has been removed from the features\n",
    "print(\"First few rows of X_train:\")\n",
    "print(X_train.head())\n",
    "# Select only the numerical features\n",
    "numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "# Compute the correlation matrix for the numerical features\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Find the two features with the highest correlation (ignoring self-correlation)\n",
    "correlation_matrix_abs = correlation_matrix.abs()  # Get the absolute values\n",
    "np.fill_diagonal(correlation_matrix_abs.values, 0)  # Fill diagonal with 0s to ignore self-correlation\n",
    "max_correlation = correlation_matrix_abs.unstack().idxmax()\n",
    "\n",
    "print(f\"\\nThe two features with the highest correlation are: {max_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8bf40d0-ddda-493d-8360-ca5e11937ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 27126\n",
      "Validation set size: 9042\n",
      "Test set size: 9043\n",
      "First few rows of X_train after encoding:\n",
      "       age  balance  day  duration  campaign  pdays  previous  \\\n",
      "6377    45     -100   27       240         6     -1         0   \n",
      "17236   29      166   28       108         8     -1         0   \n",
      "4490    31      121   20       187         1     -1         0   \n",
      "24231   40     1693   17       353         1     -1         0   \n",
      "3978    28      317   16        21         3     -1         0   \n",
      "\n",
      "       job_blue-collar  job_entrepreneur  job_housemaid  ...  month_jul  \\\n",
      "6377             False              True          False  ...      False   \n",
      "17236            False             False          False  ...       True   \n",
      "4490             False             False          False  ...      False   \n",
      "24231            False             False          False  ...      False   \n",
      "3978             False             False          False  ...      False   \n",
      "\n",
      "       month_jun  month_mar  month_may  month_nov  month_oct  month_sep  \\\n",
      "6377       False      False       True      False      False      False   \n",
      "17236      False      False      False      False      False      False   \n",
      "4490       False      False       True      False      False      False   \n",
      "24231      False      False      False       True      False      False   \n",
      "3978       False      False       True      False      False      False   \n",
      "\n",
      "       poutcome_other  poutcome_success  poutcome_unknown  \n",
      "6377            False             False              True  \n",
      "17236           False             False              True  \n",
      "4490            False             False              True  \n",
      "24231           False             False              True  \n",
      "3978            False             False              True  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 9: One-Hot Encode categorical variables\n",
    "categorical_features = ['job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# One-hot encoding the categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Step 10: Ensure the target value 'y' is not included in the feature set\n",
    "X = df_encoded.drop(columns=['y'])\n",
    "y = df_encoded['y']\n",
    "\n",
    "# Step 11: Split data into train (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Check the first few rows of the encoded data\n",
    "print(\"First few rows of X_train after encoding:\")\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2df1e-f394-4fb4-ac49-901140dabcc4",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7677b61a-849f-4bf6-9d7a-737ebb9abb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information scores for each categorical variable:\n",
      "education_secondary: 0.0\n",
      "education_tertiary: 0.0\n",
      "education_unknown: 0.0\n",
      "housing_yes: 0.01\n",
      "contact_telephone: 0.0\n",
      "contact_unknown: 0.01\n",
      "poutcome_other: 0.0\n",
      "poutcome_success: 0.03\n",
      "poutcome_unknown: 0.01\n",
      "\n",
      "The categorical variable with the highest mutual information score is: poutcome_success with a score of 0.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Step 1: Select only the categorical variables from the training set\n",
    "categorical_vars = ['contact', 'education', 'housing', 'poutcome']\n",
    "\n",
    "# We already one-hot encoded these in the previous steps, so we can select their respective one-hot encoded columns\n",
    "encoded_categorical_vars = [col for col in X_train.columns if any(var in col for var in categorical_vars)]\n",
    "\n",
    "# Step 2: Calculate the mutual information between y and each of the categorical variables\n",
    "mi_scores = mutual_info_classif(X_train[encoded_categorical_vars], y_train, discrete_features=True)\n",
    "\n",
    "# Step 3: Round the scores to 2 decimal places\n",
    "mi_scores_rounded = [round(score, 2) for score in mi_scores]\n",
    "\n",
    "# Step 4: Create a dictionary to map variables to their mutual information scores\n",
    "mi_dict = dict(zip(encoded_categorical_vars, mi_scores_rounded))\n",
    "\n",
    "# Step 5: Find the categorical variable with the highest mutual information score\n",
    "highest_mi_variable = max(mi_dict, key=mi_dict.get)\n",
    "highest_mi_score = mi_dict[highest_mi_variable]\n",
    "\n",
    "# Print the mutual information scores for each categorical variable\n",
    "print(\"Mutual Information scores for each categorical variable:\")\n",
    "for var, score in mi_dict.items():\n",
    "    print(f\"{var}: {score}\")\n",
    "\n",
    "print(f\"\\nThe categorical variable with the highest mutual information score is: {highest_mi_variable} with a score of {highest_mi_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "772bb928-65a1-46c8-b0b8-929b1cd1679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation dataset: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Initialize the logistic regression model with the specified parameters\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 2: Train the model on the training data (X_train and y_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Step 4: Calculate the accuracy on the validation set\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Step 5: Round the accuracy to 2 decimal places\n",
    "accuracy_rounded = round(accuracy, 2)\n",
    "\n",
    "# Step 6: Print the accuracy on the validation dataset\n",
    "print(f\"Accuracy on the validation dataset: {accuracy_rounded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0935b298-aecd-482f-9b29-2347c747be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without age: 0.9, Difference: -0.0009\n",
      "Accuracy without balance: 0.9, Difference: -0.0005\n",
      "Accuracy without marital: 0.9, Difference: 0.0001\n",
      "Accuracy without previous: 0.9, Difference: -0.0009\n",
      "\n",
      "The feature with the smallest difference is: age with a difference of -0.0009\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the accuracy differences\n",
    "accuracy_differences = {}\n",
    "\n",
    "# Iterate over each feature, remove the associated one-hot encoded columns, and calculate the accuracy\n",
    "for feature in features_to_test:\n",
    "    # Find all one-hot encoded columns related to the feature\n",
    "    feature_cols = [col for col in X_train.columns if col.startswith(feature)]\n",
    "    \n",
    "    # Drop the feature's one-hot encoded columns from the training and validation sets\n",
    "    X_train_reduced = X_train.drop(columns=feature_cols)\n",
    "    X_val_reduced = X_val.drop(columns=feature_cols)\n",
    "    \n",
    "    # Train a new logistic regression model without this feature\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_val_pred_reduced = model.predict(X_val_reduced)\n",
    "    \n",
    "    # Calculate the accuracy without this feature\n",
    "    accuracy_reduced = accuracy_score(y_val, y_val_pred_reduced)\n",
    "    \n",
    "    # Calculate the difference from the original accuracy\n",
    "    accuracy_diff = original_accuracy - accuracy_reduced\n",
    "    accuracy_differences[feature] = round(accuracy_diff, 4)  # Round to 4 decimal places\n",
    "    \n",
    "    # Print the result for each feature\n",
    "    print(f\"Accuracy without {feature}: {round(accuracy_reduced, 2)}, Difference: {round(accuracy_diff, 4)}\")\n",
    "\n",
    "# Find the feature with the smallest difference\n",
    "least_important_feature = min(accuracy_differences, key=accuracy_differences.get)\n",
    "smallest_difference = accuracy_differences[least_important_feature]\n",
    "\n",
    "print(f\"\\nThe feature with the smallest difference is: {least_important_feature} with a difference of {smallest_difference}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0ddee3-d3fd-4417-91e9-897131b81f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=0.01: 0.898\n",
      "Accuracy with C=0.1: 0.9\n",
      "Accuracy with C=1: 0.901\n",
      "Accuracy with C=10: 0.9\n",
      "Accuracy with C=100: 0.9\n",
      "\n",
      "The best C value is: 1 with an accuracy of 0.901\n"
     ]
    }
   ],
   "source": [
    "# List of C values to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Dictionary to store the accuracy for each value of C\n",
    "accuracy_results = {}\n",
    "\n",
    "# Train logistic regression models for each value of C and evaluate accuracy\n",
    "for C in C_values:\n",
    "    # Initialize the logistic regression model with the current value of C\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate the accuracy on the validation set\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Round the accuracy to 3 decimal places\n",
    "    accuracy_rounded = round(accuracy, 3)\n",
    "    \n",
    "    # Store the accuracy in the dictionary\n",
    "    accuracy_results[C] = accuracy_rounded\n",
    "    \n",
    "    # Print the accuracy for the current value of C\n",
    "    print(f\"Accuracy with C={C}: {accuracy_rounded}\")\n",
    "\n",
    "# Find the C value with the best accuracy\n",
    "best_C = max(accuracy_results, key=accuracy_results.get)\n",
    "best_accuracy = accuracy_results[best_C]\n",
    "\n",
    "print(f\"\\nThe best C value is: {best_C} with an accuracy of {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb77f2b-1f08-46e1-a9b2-6a6d70730705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
