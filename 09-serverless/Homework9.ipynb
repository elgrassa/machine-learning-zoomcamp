{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292b1ddf-9a26-49f4-9a4d-2944020bd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05c5749-1579-46ee-bcce-a20c86ab9808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-11 20:37:46--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/model_2024_hairstyle.keras\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/426348925/df5735c1-9082-4b67-968e-866f268793f8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241211T193746Z&X-Amz-Expires=300&X-Amz-Signature=88a4aa20060ec87f76be74186549200947f0052d706fbb8bcc7edc4e277c3543&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmodel_2024_hairstyle.keras&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-12-11 20:37:46--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/426348925/df5735c1-9082-4b67-968e-866f268793f8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241211T193746Z&X-Amz-Expires=300&X-Amz-Signature=88a4aa20060ec87f76be74186549200947f0052d706fbb8bcc7edc4e277c3543&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmodel_2024_hairstyle.keras&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 160610502 (153M) [application/octet-stream]\n",
      "Saving to: ‘model_2024_hairstyle.keras’\n",
      "\n",
      "model_2024_hairstyl 100%[===================>] 153,17M  9,14MB/s    in 18s     \n",
      "\n",
      "2024-12-11 20:38:04 (8,67 MB/s) - ‘model_2024_hairstyle.keras’ saved [160610502/160610502]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_url = \"https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/model_2024_hairstyle.keras\"\n",
    "model_path = \"model_2024_hairstyle.keras\"\n",
    "if not os.path.exists(model_path):\n",
    "    !wget -O {model_path} {model_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3a76f2-b964-4ee7-b895-062793fa6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e62760-5bb8-4fa6-bad0-0cf67525d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  6115580560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6115581328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6115582096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6115582864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6115584016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6115584592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1733945912.861644 24080479 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1733945912.862255 24080479 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-12-11 20:38:32.863298: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_\n",
      "2024-12-11 20:38:32.863665: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-12-11 20:38:32.863669: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_\n",
      "2024-12-11 20:38:32.866633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-12-11 20:38:32.867146: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-12-11 20:38:32.965229: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/1x/gz62bmh57pb8ph210c3v_9g80000gn/T/tmp3zverlt_\n",
      "2024-12-11 20:38:32.969650: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 106353 microseconds.\n",
      "2024-12-11 20:38:32.991197: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TF-Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b3be9c-a0ea-4dff-a186-e4a5a06bc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-Lite model\n",
    "tflite_model_path = \"model_2024_hairstyle.tflite\"\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6fb4fb-b283-43aa-8e54-a2a3055411fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the converted TF-Lite model: 76.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Size of the TF-Lite model in MB\n",
    "model_size_mb = os.path.getsize(tflite_model_path) / (1024 * 1024)\n",
    "print(f\"Size of the converted TF-Lite model: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7eb87-82ab-4756-a258-f914f11af551",
   "metadata": {},
   "source": [
    "Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99356614-d385-4580-a7fb-be37200e7505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input index: 0\n",
      "Output index: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_2024_hairstyle.tflite\")\n",
    "\n",
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Output index\n",
    "print(f\"Input index: {input_details[0]['index']}\")\n",
    "print(f\"Output index: {output_details[0]['index']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae5e17-5e8e-4a91-9066-0685ab2d4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e8f1d3-7a59-4bb5-89e7-1928bb7da606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (1, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Functions for downloading and preparing the image\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "    # Normalize the image to the range [0, 1]\n",
    "    img_array /= 255.0\n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Download, resize, and preprocess the image\n",
    "image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "image = download_image(image_url)\n",
    "prepared_image = prepare_image(image, (200, 200))\n",
    "input_data = preprocess_image(prepared_image)\n",
    "\n",
    "print(f\"Input data shape: {input_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "599a8293-3def-4e88-b18e-aabeff0fadd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.8934686]]\n",
      "Model output: [[0.8934686]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_2024_hairstyle.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Retrieve input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set the input tensor using the input index\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Retrieve the output using the output index (13)\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Model output: {output_data}\")\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_2024_hairstyle.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Retrieve input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set the input tensor using the input index\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Retrieve the output using the output index (13)\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Model output: {output_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d4a7128-7bc7-430c-8fe3-4f723fc092ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "predicted_class = 1 if output_data[0][0] > threshold else 0\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5f28a-53f6-463e-b457-4c9c1ad31bee",
   "metadata": {},
   "source": [
    "Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82fd6d4e-686b-4bb6-9697-4d411343e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the first pixel's R channel: 0.24\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Download the image\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "# Resize the image to the target size\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# Download and resize the image\n",
    "image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "image = download_image(image_url)\n",
    "prepared_image = prepare_image(image, (200, 200))\n",
    "\n",
    "# Convert to NumPy array and normalize\n",
    "img_array = np.array(prepared_image, dtype=np.float32) / 255.0\n",
    "\n",
    "# Extract the R channel value of the first pixel\n",
    "first_pixel_r_channel = img_array[0, 0, 0]\n",
    "print(f\"Value of the first pixel's R channel: {first_pixel_r_channel:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063eda2-0f46-4138-a062-7b096cec3fd8",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9c76ad4-5736-4cdc-98b5-e8b3d55f97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: 0.893\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Download and preprocess the image\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Convert the image to a NumPy array and normalize\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Load the TF-Lite model and make a prediction\n",
    "def predict_with_model(image_url, model_path):\n",
    "    # Download and prepare the image\n",
    "    image = download_image(image_url)\n",
    "    prepared_image = prepare_image(image, (200, 200))\n",
    "    input_data = preprocess_image(prepared_image)\n",
    "    \n",
    "    # Load the model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Set the input tensor and invoke the model\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data[0][0]\n",
    "\n",
    "# Define the image and model path\n",
    "image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "model_path = \"model_2024_hairstyle.tflite\"  # Path to the TF-Lite model\n",
    "\n",
    "# Predict\n",
    "output = predict_with_model(image_url, model_path)\n",
    "print(f\"Model output: {output:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6e9b1-ac3a-4ee6-9868-91112808e9b8",
   "metadata": {},
   "source": [
    "docker images | grep hairstyle-lambda\n",
    "# printed 890MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7917500-3e6c-446b-8537-0ad55d577e1c",
   "metadata": {},
   "source": [
    "Question6 : for mac had to use : docker build --platform linux/arm64 -t hairstyle-lambda .\n",
    "issues with ports, should be 0.829 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9727b3a6-4855-40b0-9461-87e487fc01c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1648387167.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    lambda_model.py:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tflite_runtime.interpreter import Interpreter  # Use tflite_runtime\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"Download an image from the specified URL.\"\"\"\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size=(200, 200)):\n",
    "    \"\"\"Resize and convert the image to RGB.\"\"\"\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Convert image to NumPy array and normalize.\"\"\"\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "def predict_with_model(image_url, model_path=\"/var/task/model_2024_hairstyle_v2.tflite\"):\n",
    "    \"\"\"Run inference using the TF-Lite model.\"\"\"\n",
    "    print(\"Downloading and preparing image...\")\n",
    "    image = download_image(image_url)\n",
    "    prepared_image = prepare_image(image)\n",
    "    input_data = preprocess_image(prepared_image)\n",
    "    \n",
    "    print(\"Loading TF-Lite model...\")\n",
    "    interpreter = Interpreter(model_path=model_path)  # Use tflite_runtime.Interpreter\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    print(\"Fetching output...\")\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data[0][0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the Lambda locally with the specified image URL\n",
    "    image_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "    output = predict_with_model(image_url)\n",
    "    print(f\"Model output: {output:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8114c70-145a-4db2-8517-5f169561699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM agrigorev/model-2024-hairstyle:v3\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /var/task\n",
    "\n",
    "# Copy the Lambda model script into /var/task/\n",
    "COPY ./lambda_model.py /var/task/\n",
    "\n",
    "# Debugging: Check if the file exists in /var/task/\n",
    "RUN ls -l /var/task/\n",
    "\n",
    "# Install additional required libraries\n",
    "RUN pip install numpy==1.23.2\n",
    "RUN pip install Pillow\n",
    "RUN pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "# Explicitly set ENTRYPOINT to Python\n",
    "ENTRYPOINT [\"python3\"]\n",
    "\n",
    "# Default command to run the Lambda script\n",
    "CMD [\"lambda_model.py\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a94d1d-9210-48b5-bc61-e7c5aff2a0ea",
   "metadata": {},
   "source": [
    "Docker logs:\n",
    "What's next:\n",
    "    View a summary of image vulnerabilities and recommendations → docker scout quickview \n",
    "(base) ➜  ml docker run -it --entrypoint sh hairstyle-lambda-extended\n",
    "\n",
    "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
    "sh-4.2# python3 /var/task/lambda_model.py\n",
    "Downloading and preparing image...\n",
    "Loading TF-Lite model...\n",
    "\n",
    "A module that was compiled using NumPy 1.x cannot be run in\n",
    "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
    "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
    "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
    "\n",
    "If you are a user of the module, the easiest solution will be to\n",
    "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
    "We expect that some modules will need time to support NumPy 2.\n",
    "\n",
    "Traceback (most recent call last):  File \"/var/task/lambda_model.py\", line 52, in <module>\n",
    "    output = predict_with_model(image_url)\n",
    "  File \"/var/task/lambda_model.py\", line 36, in predict_with_model\n",
    "    interpreter = Interpreter(model_path=model_path)  # Use tflite_runtime.Interpreter\n",
    "  File \"/var/lang/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 464, in __init__\n",
    "    self._interpreter = _interpreter_wrapper.CreateWrapperFromFile(\n",
    "Traceback (most recent call last):\n",
    "  File \"/var/lang/lib/python3.10/site-packages/numpy/core/_multiarray_umath.py\", line 44, in __getattr__\n",
    "    raise ImportError(msg)\n",
    "ImportError: \n",
    "A module that was compiled using NumPy 1.x cannot be run in\n",
    "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
    "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
    "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
    "\n",
    "If you are a user of the module, the easiest solution will be to\n",
    "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
    "We expect that some modules will need time to support NumPy 2.\n",
    "\n",
    "\n",
    "ImportError: numpy.core.multiarray failed to import\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/var/task/lambda_model.py\", line 52, in <module>\n",
    "    output = predict_with_model(image_url)\n",
    "  File \"/var/task/lambda_model.py\", line 36, in predict_with_model\n",
    "    interpreter = Interpreter(model_path=model_path)  # Use tflite_runtime.Interpreter\n",
    "  File \"/var/lang/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 464, in __init__\n",
    "    self._interpreter = _interpreter_wrapper.CreateWrapperFromFile(\n",
    "SystemError: <built-in method CreateWrapperFromFile of PyCapsule object at 0x7fffdc8a1260> returned a result with an exception set\n",
    "sh-4.2# pip install numpy==1.23.1 --force-reinstall\n",
    "Collecting numpy==1.23.1\n",
    "  Using cached numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
    "Installing collected packages: numpy\n",
    "  Attempting uninstall: numpy\n",
    "    Found existing installation: numpy 2.2.0\n",
    "    Uninstalling numpy-2.2.0:\n",
    "      Successfully uninstalled numpy-2.2.0\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "tflite-runtime 2.14.0 requires numpy>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n",
    "Successfully installed numpy-1.23.1\n",
    "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
    "\n",
    "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
    "[notice] To update, run: pip install --upgrade pip\n",
    "sh-4.2# pip install numpy==1.23.2 --force-reinstall\n",
    "Collecting numpy==1.23.2\n",
    "  Downloading numpy-1.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 5.9 MB/s eta 0:00:00\n",
    "Installing collected packages: numpy\n",
    "  Attempting uninstall: numpy\n",
    "    Found existing installation: numpy 1.23.1\n",
    "    Uninstalling numpy-1.23.1:\n",
    "      Successfully uninstalled numpy-1.23.1\n",
    "Successfully installed numpy-1.23.2\n",
    "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
    "\n",
    "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
    "[notice] To update, run: pip install --upgrade pip\n",
    "sh-4.2# python3 /var/task/lambda_model.py\n",
    "Downloading and preparing image...\n",
    "Loading TF-Lite model...\n",
    "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
    "Running inference...\n",
    "Fetching output...\n",
    "Model output: 0.430"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
